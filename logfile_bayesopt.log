2024-03-11 19:36:58,358 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-11 19:38:02,785 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-11 19:38:02,804 - INFO - Starting objective function execution.
2024-03-11 19:38:02,806 - INFO - Received attributes: learning_rate=0.00024400607090817544, weight_decay=6.071989493441303e-05, batch_size=2, num_epochs=3, max_sequence_len=230
2024-03-11 19:38:02,806 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-11 19:38:07,328 - INFO - Tokenizer initialized successfully.
2024-03-11 19:38:07,329 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-11 19:38:12,870 - INFO - NER Dataset prepared successfully.
2024-03-11 19:38:12,870 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-11 19:38:28,166 - INFO - Model initialized successfully.
2024-03-11 19:38:28,168 - INFO - Training...
2024-03-11 19:38:39,750 - INFO - Training completed.
2024-03-11 19:38:39,750 - INFO - Evaluation...
2024-03-11 19:38:40,808 - INFO - Evaluation completed.
2024-03-11 19:38:41,522 - INFO - Objective function execution completed in 38.72 seconds.
2024-03-11 19:38:41,524 - INFO - Starting objective function execution.
2024-03-11 19:38:41,526 - INFO - Received attributes: learning_rate=3.727925903376984e-06, weight_decay=0.00040137830238462545, batch_size=2, num_epochs=2, max_sequence_len=160
2024-03-11 19:38:41,526 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-11 19:38:42,871 - INFO - Tokenizer initialized successfully.
2024-03-11 19:38:42,872 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-11 19:38:48,459 - INFO - NER Dataset prepared successfully.
2024-03-11 19:38:48,460 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-11 19:38:50,151 - INFO - Model initialized successfully.
2024-03-11 19:38:50,151 - INFO - Training...
2024-03-11 19:38:58,228 - INFO - Training completed.
2024-03-11 19:38:58,229 - INFO - Evaluation...
2024-03-11 19:38:59,349 - INFO - Evaluation completed.
2024-03-11 19:39:00,155 - INFO - Objective function execution completed in 18.63 seconds.
2024-03-11 19:39:00,158 - INFO - Starting objective function execution.
2024-03-11 19:39:00,159 - INFO - Received attributes: learning_rate=1.0071984838809193e-06, weight_decay=0.009307782732027245, batch_size=3, num_epochs=3, max_sequence_len=156
2024-03-11 19:39:00,159 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-11 19:39:01,870 - INFO - Tokenizer initialized successfully.
2024-03-11 19:39:01,871 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-11 19:39:08,529 - INFO - NER Dataset prepared successfully.
2024-03-11 19:39:08,530 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-11 19:39:10,395 - INFO - Model initialized successfully.
2024-03-11 19:39:10,395 - INFO - Training...
2024-03-11 19:39:23,717 - INFO - Training completed.
2024-03-11 19:39:23,718 - INFO - Evaluation...
2024-03-11 19:39:24,788 - INFO - Evaluation completed.
2024-03-11 19:39:25,531 - INFO - Objective function execution completed in 25.37 seconds.
2024-03-11 19:39:25,533 - INFO - Starting objective function execution.
2024-03-11 19:39:25,534 - INFO - Received attributes: learning_rate=1.2366582530130827e-06, weight_decay=0.00012563152773938674, batch_size=3, num_epochs=1, max_sequence_len=212
2024-03-11 19:39:25,534 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-11 19:39:26,881 - INFO - Tokenizer initialized successfully.
2024-03-11 19:39:26,882 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 00:05:57,206 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-12 00:05:57,221 - INFO - Starting objective function execution.
2024-03-12 00:05:57,223 - INFO - Received attributes: learning_rate=0.00024400607090817544, weight_decay=6.071989493441303e-05, batch_size=4, num_epochs=8, max_sequence_len=309
2024-03-12 00:05:57,224 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 00:05:58,444 - INFO - Tokenizer initialized successfully.
2024-03-12 00:05:58,445 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 00:06:02,887 - INFO - NER Dataset prepared successfully.
2024-03-12 00:06:02,888 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 00:06:06,686 - INFO - Model initialized successfully.
2024-03-12 00:06:06,687 - INFO - Training...
2024-03-12 00:19:42,255 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-12 00:19:42,284 - INFO - Starting objective function execution.
2024-03-12 00:19:42,285 - INFO - Received attributes: learning_rate=0.00024400607090817544, weight_decay=6.071989493441303e-05, batch_size=4, num_epochs=4, max_sequence_len=230
2024-03-12 00:19:42,285 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 00:19:43,512 - INFO - Tokenizer initialized successfully.
2024-03-12 00:19:43,513 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 00:19:47,621 - INFO - NER Dataset prepared successfully.
2024-03-12 00:19:47,622 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 00:19:51,309 - INFO - Model initialized successfully.
2024-03-12 00:19:51,310 - INFO - Training...
2024-03-12 00:32:05,834 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-12 00:32:05,854 - INFO - Starting objective function execution.
2024-03-12 00:32:05,855 - INFO - Received attributes: learning_rate=0.00024400607090817544, weight_decay=6.071989493441303e-05, batch_size=4, num_epochs=4, max_sequence_len=230
2024-03-12 00:32:05,856 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 00:32:07,053 - INFO - Tokenizer initialized successfully.
2024-03-12 00:32:07,054 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 00:32:11,145 - INFO - NER Dataset prepared successfully.
2024-03-12 00:32:11,147 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 00:32:14,949 - INFO - Model initialized successfully.
2024-03-12 00:32:14,950 - INFO - Training...
2024-03-12 00:48:53,946 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-12 00:48:53,971 - INFO - Starting objective function execution.
2024-03-12 00:48:53,973 - INFO - Received attributes: learning_rate=0.00024400607090817544, weight_decay=6.071989493441303e-05, batch_size=4, num_epochs=4, max_sequence_len=230
2024-03-12 00:48:53,974 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 00:48:55,194 - INFO - Tokenizer initialized successfully.
2024-03-12 00:48:55,195 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 00:48:59,343 - INFO - NER Dataset prepared successfully.
2024-03-12 00:48:59,344 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 00:49:03,116 - INFO - Model initialized successfully.
2024-03-12 00:49:03,117 - INFO - Training...
2024-03-12 01:17:06,029 - INFO - Training completed.
2024-03-12 01:17:06,030 - INFO - Evaluation...
2024-03-12 01:17:11,563 - INFO - Evaluation completed.
2024-03-12 01:17:12,115 - INFO - Objective function execution completed in 1698.14 seconds.
2024-03-12 01:17:12,118 - INFO - Starting objective function execution.
2024-03-12 01:17:12,119 - INFO - Received attributes: learning_rate=3.727925903376984e-06, weight_decay=0.00040137830238462545, batch_size=8, num_epochs=2, max_sequence_len=160
2024-03-12 01:17:12,120 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 01:17:13,334 - INFO - Tokenizer initialized successfully.
2024-03-12 01:17:13,335 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 01:17:17,727 - INFO - NER Dataset prepared successfully.
2024-03-12 01:17:17,729 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 01:17:19,257 - INFO - Model initialized successfully.
2024-03-12 01:17:19,259 - INFO - Training...
2024-03-12 01:25:07,649 - INFO - Training completed.
2024-03-12 01:25:07,658 - INFO - Evaluation...
2024-03-12 01:25:11,241 - INFO - Evaluation completed.
2024-03-12 01:25:11,751 - INFO - Objective function execution completed in 479.63 seconds.
2024-03-12 01:25:11,753 - INFO - Starting objective function execution.
2024-03-12 01:25:11,754 - INFO - Received attributes: learning_rate=1.0071984838809193e-06, weight_decay=0.009307782732027245, batch_size=12, num_epochs=5, max_sequence_len=156
2024-03-12 01:25:11,756 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 01:25:12,704 - INFO - Tokenizer initialized successfully.
2024-03-12 01:25:12,705 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 01:25:16,906 - INFO - NER Dataset prepared successfully.
2024-03-12 01:25:16,907 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 01:25:18,703 - INFO - Model initialized successfully.
2024-03-12 01:25:18,704 - INFO - Training...
2024-03-12 01:42:21,995 - INFO - Training completed.
2024-03-12 01:42:22,012 - INFO - Evaluation...
2024-03-12 01:42:25,648 - INFO - Evaluation completed.
2024-03-12 01:42:26,159 - INFO - Objective function execution completed in 1034.41 seconds.
2024-03-12 01:42:26,161 - INFO - Starting objective function execution.
2024-03-12 01:42:26,162 - INFO - Received attributes: learning_rate=1.2366582530130827e-06, weight_decay=0.00012563152773938674, batch_size=10, num_epochs=1, max_sequence_len=212
2024-03-12 01:42:26,163 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 01:42:27,169 - INFO - Tokenizer initialized successfully.
2024-03-12 01:42:27,170 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 01:42:31,535 - INFO - NER Dataset prepared successfully.
2024-03-12 01:42:31,536 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 01:42:33,082 - INFO - Model initialized successfully.
2024-03-12 01:42:33,083 - INFO - Training...
2024-03-12 01:47:02,286 - INFO - Training completed.
2024-03-12 01:47:02,297 - INFO - Evaluation...
2024-03-12 01:47:06,817 - INFO - Evaluation completed.
2024-03-12 01:47:07,370 - INFO - Objective function execution completed in 281.21 seconds.
2024-03-12 01:47:07,372 - INFO - Starting objective function execution.
2024-03-12 01:47:07,374 - INFO - Received attributes: learning_rate=8.532678095658718e-06, weight_decay=2.3036990230378637e-06, batch_size=2, num_epochs=5, max_sequence_len=190
2024-03-12 01:47:07,375 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 01:47:08,362 - INFO - Tokenizer initialized successfully.
2024-03-12 01:47:08,363 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 01:47:12,719 - INFO - NER Dataset prepared successfully.
2024-03-12 01:47:12,722 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 01:47:14,213 - INFO - Model initialized successfully.
2024-03-12 01:47:14,214 - INFO - Training...
2024-03-12 02:37:08,877 - INFO - Training completed.
2024-03-12 02:37:08,891 - INFO - Evaluation...
2024-03-12 02:37:14,665 - INFO - Evaluation completed.
2024-03-12 02:37:15,233 - INFO - Objective function execution completed in 3007.86 seconds.
2024-03-12 02:37:15,235 - INFO - Starting objective function execution.
2024-03-12 02:37:15,235 - INFO - Received attributes: learning_rate=7.362945281639229e-05, weight_decay=0.0027527173929429443, batch_size=7, num_epochs=5, max_sequence_len=212
2024-03-12 02:37:15,236 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 02:37:16,325 - INFO - Tokenizer initialized successfully.
2024-03-12 02:37:16,326 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 02:37:20,689 - INFO - NER Dataset prepared successfully.
2024-03-12 02:37:20,690 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 02:37:22,243 - INFO - Model initialized successfully.
2024-03-12 02:37:22,244 - INFO - Training...
2024-03-12 03:02:46,502 - INFO - Training completed.
2024-03-12 03:02:46,525 - INFO - Evaluation...
2024-03-12 03:02:51,258 - INFO - Evaluation completed.
2024-03-12 03:02:51,815 - INFO - Objective function execution completed in 1536.58 seconds.
2024-03-12 03:02:51,817 - INFO - Starting objective function execution.
2024-03-12 03:02:51,817 - INFO - Received attributes: learning_rate=0.005872283616443731, weight_decay=0.00017912362571043672, batch_size=8, num_epochs=1, max_sequence_len=218
2024-03-12 03:02:51,818 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 03:02:52,764 - INFO - Tokenizer initialized successfully.
2024-03-12 03:02:52,765 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 03:02:57,168 - INFO - NER Dataset prepared successfully.
2024-03-12 03:02:57,169 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 03:02:58,658 - INFO - Model initialized successfully.
2024-03-12 03:02:58,659 - INFO - Training...
2024-03-12 03:07:57,434 - INFO - Training completed.
2024-03-12 03:07:57,446 - INFO - Evaluation...
2024-03-12 03:08:02,265 - INFO - Evaluation completed.
2024-03-12 03:08:02,902 - INFO - Objective function execution completed in 311.09 seconds.
2024-03-12 03:08:02,904 - INFO - Starting objective function execution.
2024-03-12 03:08:02,905 - INFO - Received attributes: learning_rate=9.206654892274771e-06, weight_decay=0.0005408216580718496, batch_size=1, num_epochs=2, max_sequence_len=189
2024-03-12 03:08:02,906 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 03:08:03,830 - INFO - Tokenizer initialized successfully.
2024-03-12 03:08:03,831 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 03:08:08,066 - INFO - NER Dataset prepared successfully.
2024-03-12 03:08:08,067 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 03:08:09,592 - INFO - Model initialized successfully.
2024-03-12 03:08:09,593 - INFO - Training...
2024-03-12 03:46:55,228 - INFO - Training completed.
2024-03-12 03:46:55,242 - INFO - Evaluation...
2024-03-12 03:47:05,326 - INFO - Evaluation completed.
2024-03-12 03:47:05,890 - INFO - Objective function execution completed in 2342.99 seconds.
2024-03-12 03:47:05,892 - INFO - Starting objective function execution.
2024-03-12 03:47:05,892 - INFO - Received attributes: learning_rate=3.666421832063727e-05, weight_decay=5.3572800696018355e-06, batch_size=13, num_epochs=2, max_sequence_len=211
2024-03-12 03:47:05,893 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 03:47:06,923 - INFO - Tokenizer initialized successfully.
2024-03-12 03:47:06,924 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 03:47:11,310 - INFO - NER Dataset prepared successfully.
2024-03-12 03:47:11,312 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 03:47:12,807 - INFO - Model initialized successfully.
2024-03-12 03:47:12,808 - INFO - Training...
2024-03-12 03:55:38,404 - INFO - Training completed.
2024-03-12 03:55:38,423 - INFO - Evaluation...
2024-03-12 03:55:42,921 - INFO - Evaluation completed.
2024-03-12 03:55:43,508 - INFO - Objective function execution completed in 517.62 seconds.
2024-03-12 03:55:43,510 - INFO - Starting objective function execution.
2024-03-12 03:55:43,510 - INFO - Received attributes: learning_rate=0.00018655260217376844, weight_decay=1.3342990285187939e-06, batch_size=7, num_epochs=2, max_sequence_len=226
2024-03-12 03:55:43,511 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-12 03:55:44,494 - INFO - Tokenizer initialized successfully.
2024-03-12 03:55:44,496 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-12 03:55:48,781 - INFO - NER Dataset prepared successfully.
2024-03-12 03:55:48,782 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-12 03:55:50,408 - INFO - Model initialized successfully.
2024-03-12 03:55:50,409 - INFO - Training...
2024-03-12 04:06:10,477 - INFO - Training completed.
2024-03-12 04:06:10,491 - INFO - Evaluation...
2024-03-12 04:06:15,195 - INFO - Evaluation completed.
2024-03-12 04:06:15,738 - INFO - Objective function execution completed in 632.23 seconds.
2024-03-12 04:06:16,286 - INFO - End: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-12 04:06:16,287 - INFO - Saving the scikit-optimize (bayesian optimization) results w/ filename
2024-03-14 13:44:52,920 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-14 13:44:52,949 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 13:44:52,951 - INFO - Received attributes: batch_size=4, num_epochs=8, max_sequence_len=82, input_dim: 200, hidden_dim: 256, n_layer: 1, dropout: 0.22962444598293363, learning_rate=2.1618942406574448e-05, weight_decay=3.727925903376984e-06
2024-03-14 13:44:52,954 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 13:45:20,578 - INFO - NER Dataset prepared successfully.
2024-03-14 13:45:20,581 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 13:45:20,583 - INFO - Model configuration dict prepared successfully.
2024-03-14 13:45:20,585 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 13:45:24,537 - INFO - Model initialized successfully.
2024-03-14 13:45:24,540 - INFO - Training...
2024-03-14 13:56:35,939 - INFO - Training completed.
2024-03-14 13:56:35,942 - INFO - Evaluation...
2024-03-14 13:56:37,671 - INFO - Measured `f1` score: 0.33163698049194235
2024-03-14 13:56:37,674 - INFO - Evaluation completed.
2024-03-14 13:56:37,941 - INFO - Objective function execution completed in 704.99 seconds.
2024-03-14 13:56:37,992 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 13:56:37,995 - INFO - Received attributes: batch_size=2, num_epochs=7, max_sequence_len=69, input_dim: 300, hidden_dim: 64, n_layer: 5, dropout: 0.3087407548138583, learning_rate=0.0002796485951606247, weight_decay=1.0672476836323728e-06
2024-03-14 13:56:37,997 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 13:57:14,109 - INFO - NER Dataset prepared successfully.
2024-03-14 13:57:14,110 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 13:57:14,111 - INFO - Model configuration dict prepared successfully.
2024-03-14 13:57:14,112 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 13:57:15,272 - INFO - Model initialized successfully.
2024-03-14 13:57:15,274 - INFO - Training...
2024-03-14 14:43:03,361 - INFO - Training completed.
2024-03-14 14:43:03,387 - INFO - Evaluation...
2024-03-14 14:43:08,777 - INFO - Measured `f1` score: 0.4411650485436893
2024-03-14 14:43:08,779 - INFO - Evaluation completed.
2024-03-14 14:43:09,107 - INFO - Objective function execution completed in 2791.11 seconds.
2024-03-14 14:43:09,174 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 14:43:09,176 - INFO - Received attributes: batch_size=9, num_epochs=5, max_sequence_len=12, input_dim: 50, hidden_dim: 768, n_layer: 2, dropout: 0.04530321726641041, learning_rate=0.0002975390947349387, weight_decay=3.387255565852147e-05
2024-03-14 14:43:09,177 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 14:43:21,796 - INFO - NER Dataset prepared successfully.
2024-03-14 14:43:21,801 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 14:43:21,806 - INFO - Model configuration dict prepared successfully.
2024-03-14 14:43:21,809 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 14:43:22,239 - INFO - Model initialized successfully.
2024-03-14 14:43:22,244 - INFO - Training...
2024-03-14 14:51:24,878 - INFO - Training completed.
2024-03-14 14:51:24,880 - INFO - Evaluation...
2024-03-14 14:51:27,390 - INFO - Measured `f1` score: 0.5019425019425019
2024-03-14 14:51:27,391 - INFO - Evaluation completed.
2024-03-14 14:51:27,677 - INFO - Objective function execution completed in 498.50 seconds.
2024-03-14 14:51:27,692 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 14:51:27,693 - INFO - Received attributes: batch_size=8, num_epochs=9, max_sequence_len=98, input_dim: 200, hidden_dim: 256, n_layer: 1, dropout: 0.4711008778424265, learning_rate=0.00017912362571043672, weight_decay=3.4806953267852694e-05
2024-03-14 14:51:27,693 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 14:51:54,503 - INFO - NER Dataset prepared successfully.
2024-03-14 14:51:54,504 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 14:51:54,505 - INFO - Model configuration dict prepared successfully.
2024-03-14 14:51:54,506 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 14:51:55,295 - INFO - Model initialized successfully.
2024-03-14 14:51:55,297 - INFO - Training...
2024-03-14 14:58:40,638 - INFO - Training completed.
2024-03-14 14:58:40,639 - INFO - Evaluation...
2024-03-14 14:58:42,049 - INFO - Measured `f1` score: 0.48453608247422675
2024-03-14 14:58:42,051 - INFO - Evaluation completed.
2024-03-14 14:58:42,338 - INFO - Objective function execution completed in 434.65 seconds.
2024-03-14 14:58:42,370 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 14:58:42,371 - INFO - Received attributes: batch_size=4, num_epochs=3, max_sequence_len=11, input_dim: 200, hidden_dim: 512, n_layer: 4, dropout: 0.08668232675388605, learning_rate=3.666421832063727e-05, weight_decay=5.3572800696018355e-06
2024-03-14 14:58:42,372 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 14:59:08,177 - INFO - NER Dataset prepared successfully.
2024-03-14 14:59:08,178 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 14:59:08,179 - INFO - Model configuration dict prepared successfully.
2024-03-14 14:59:08,180 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 14:59:09,157 - INFO - Model initialized successfully.
2024-03-14 14:59:09,159 - INFO - Training...
2024-03-14 15:15:00,599 - INFO - Training completed.
2024-03-14 15:15:00,618 - INFO - Evaluation...
2024-03-14 15:15:06,654 - INFO - Measured `f1` score: 0.3729957805907173
2024-03-14 15:15:06,656 - INFO - Evaluation completed.
2024-03-14 15:15:06,931 - INFO - Objective function execution completed in 984.56 seconds.
2024-03-14 15:15:06,962 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 15:15:06,964 - INFO - Received attributes: batch_size=7, num_epochs=3, max_sequence_len=78, input_dim: 200, hidden_dim: 64, n_layer: 4, dropout: 0.2248770666848829, learning_rate=3.807158379249398e-05, weight_decay=0.005089035297840717
2024-03-14 15:15:06,967 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 15:15:31,335 - INFO - NER Dataset prepared successfully.
2024-03-14 15:15:31,336 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 15:15:31,338 - INFO - Model configuration dict prepared successfully.
2024-03-14 15:15:31,340 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 15:15:32,100 - INFO - Model initialized successfully.
2024-03-14 15:15:32,102 - INFO - Training...
2024-03-14 15:20:04,588 - INFO - Training completed.
2024-03-14 15:20:04,589 - INFO - Evaluation...
2024-03-14 15:20:06,983 - INFO - Measured `f1` score: 0.11234764175940648
2024-03-14 15:20:06,984 - INFO - Evaluation completed.
2024-03-14 15:20:07,285 - INFO - Objective function execution completed in 300.32 seconds.
2024-03-14 15:20:07,338 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 15:20:07,339 - INFO - Received attributes: batch_size=6, num_epochs=6, max_sequence_len=75, input_dim: 200, hidden_dim: 768, n_layer: 4, dropout: 0.3736600550686905, learning_rate=0.00014413469371110337, weight_decay=0.00022233337605920384
2024-03-14 15:20:07,340 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 15:20:30,432 - INFO - NER Dataset prepared successfully.
2024-03-14 15:20:30,434 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 15:20:30,435 - INFO - Model configuration dict prepared successfully.
2024-03-14 15:20:30,436 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 15:20:31,599 - INFO - Model initialized successfully.
2024-03-14 15:20:31,600 - INFO - Training...
2024-03-14 15:50:58,051 - INFO - Training completed.
2024-03-14 15:50:58,054 - INFO - Evaluation...
2024-03-14 15:51:03,896 - INFO - Measured `f1` score: 0.49670159099728367
2024-03-14 15:51:03,897 - INFO - Evaluation completed.
2024-03-14 15:51:04,193 - INFO - Objective function execution completed in 1856.85 seconds.
2024-03-14 15:51:04,242 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 15:51:04,243 - INFO - Received attributes: batch_size=10, num_epochs=3, max_sequence_len=97, input_dim: 100, hidden_dim: 64, n_layer: 1, dropout: 0.21170074035318487, learning_rate=3.7977473791736935e-05, weight_decay=1.4926318309209375e-05
2024-03-14 15:51:04,244 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 15:51:20,180 - INFO - NER Dataset prepared successfully.
2024-03-14 15:51:20,181 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 15:51:20,182 - INFO - Model configuration dict prepared successfully.
2024-03-14 15:51:20,182 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 15:51:20,540 - INFO - Model initialized successfully.
2024-03-14 15:51:20,544 - INFO - Training...
2024-03-14 15:52:30,635 - INFO - Training completed.
2024-03-14 15:52:30,636 - INFO - Evaluation...
2024-03-14 15:52:31,887 - INFO - Measured `f1` score: 0.0733048259010385
2024-03-14 15:52:31,888 - INFO - Evaluation completed.
2024-03-14 15:52:32,162 - INFO - Objective function execution completed in 87.92 seconds.
2024-03-14 15:52:32,188 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 15:52:32,189 - INFO - Received attributes: batch_size=4, num_epochs=7, max_sequence_len=11, input_dim: 300, hidden_dim: 512, n_layer: 5, dropout: 0.3255385127509723, learning_rate=0.004569184576834553, weight_decay=0.002512779099948731
2024-03-14 15:52:32,191 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 15:53:08,002 - INFO - NER Dataset prepared successfully.
2024-03-14 15:53:08,003 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 15:53:08,004 - INFO - Model configuration dict prepared successfully.
2024-03-14 15:53:08,005 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 15:53:09,358 - INFO - Model initialized successfully.
2024-03-14 15:53:09,359 - INFO - Training...
2024-03-14 16:43:14,749 - INFO - Training completed.
2024-03-14 16:43:14,765 - INFO - Evaluation...
2024-03-14 16:43:22,343 - INFO - Measured `f1` score: 0.0
2024-03-14 16:43:22,346 - INFO - Evaluation completed.
2024-03-14 16:43:22,689 - INFO - Objective function execution completed in 3050.50 seconds.
2024-03-14 16:43:22,762 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 16:43:22,763 - INFO - Received attributes: batch_size=2, num_epochs=4, max_sequence_len=50, input_dim: 200, hidden_dim: 512, n_layer: 3, dropout: 0.1373608964950321, learning_rate=0.00017578171745893576, weight_decay=3.401789991128542e-05
2024-03-14 16:43:22,764 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 16:43:47,385 - INFO - NER Dataset prepared successfully.
2024-03-14 16:43:47,387 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 16:43:47,389 - INFO - Model configuration dict prepared successfully.
2024-03-14 16:43:47,390 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 16:43:48,327 - INFO - Model initialized successfully.
2024-03-14 16:43:48,330 - INFO - Training...
2024-03-14 17:14:23,283 - INFO - Training completed.
2024-03-14 17:14:23,284 - INFO - Evaluation...
2024-03-14 17:14:30,947 - INFO - Measured `f1` score: 0.5291828793774319
2024-03-14 17:14:30,947 - INFO - Evaluation completed.
2024-03-14 17:14:31,249 - INFO - Objective function execution completed in 1868.49 seconds.
2024-03-14 17:14:32,062 - INFO - Starting (lstm-cnn) objective function execution.
2024-03-14 17:14:32,064 - INFO - Received attributes: batch_size=1, num_epochs=10, max_sequence_len=100, input_dim: 200, hidden_dim: 768, n_layer: 5, dropout: 0.5, learning_rate=0.0011102521771454545, weight_decay=0.01
2024-03-14 17:14:32,065 - INFO - Preparing NER Dataset: NERDataset_lstm_cnn(config=config_data, glove_dim=input_dim)
2024-03-14 17:14:56,041 - INFO - NER Dataset prepared successfully.
2024-03-14 17:14:56,042 - INFO - Preparing model configuration dict: prepare_model_config(**kwargs)
2024-03-14 17:14:56,043 - INFO - Model configuration dict prepared successfully.
2024-03-14 17:14:56,043 - INFO - Initializing Model: LSTMCNNForTokenClassification(config=lstm_cnn_model_config)
2024-03-14 17:14:57,336 - INFO - Model initialized successfully.
2024-03-14 17:14:57,337 - INFO - Training...
2024-03-15 10:35:36,511 - INFO - Start: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-15 10:35:36,527 - INFO - Starting (transformers) objective function execution.
2024-03-15 10:35:36,529 - INFO - Received attributes: batch_size=4, num_epochs=8, max_sequence_len=82, learning_rate=0.00024400607090817544, weight_decay=6.071989493441303e-05
2024-03-15 10:35:36,529 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 10:35:40,091 - INFO - Tokenizer initialized successfully.
2024-03-15 10:35:40,092 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 10:35:44,917 - INFO - NER Dataset prepared successfully.
2024-03-15 10:35:44,918 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 10:36:05,049 - INFO - Model initialized successfully.
2024-03-15 10:36:05,050 - INFO - Training...
2024-03-15 11:16:04,511 - INFO - Training completed.
2024-03-15 11:16:04,513 - INFO - Evaluation...
2024-03-15 11:16:08,080 - INFO - Measured `f1` score: 0.0
2024-03-15 11:16:08,081 - INFO - Evaluation completed.
2024-03-15 11:16:08,639 - INFO - Objective function execution completed in 2432.11 seconds.
2024-03-15 11:16:08,642 - INFO - Starting (transformers) objective function execution.
2024-03-15 11:16:08,643 - INFO - Received attributes: batch_size=8, num_epochs=4, max_sequence_len=19, learning_rate=3.727925903376984e-06, weight_decay=0.00040137830238462545
2024-03-15 11:16:08,644 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 11:16:09,698 - INFO - Tokenizer initialized successfully.
2024-03-15 11:16:09,699 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 11:16:13,923 - INFO - NER Dataset prepared successfully.
2024-03-15 11:16:13,924 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 11:16:15,923 - INFO - Model initialized successfully.
2024-03-15 11:16:15,924 - INFO - Training...
2024-03-15 11:26:06,938 - INFO - Training completed.
2024-03-15 11:26:06,939 - INFO - Evaluation...
2024-03-15 11:26:09,247 - INFO - Measured `f1` score: 0.5384241245136188
2024-03-15 11:26:09,248 - INFO - Evaluation completed.
2024-03-15 11:26:09,835 - INFO - Objective function execution completed in 601.19 seconds.
2024-03-15 11:26:09,837 - INFO - Starting (transformers) objective function execution.
2024-03-15 11:26:09,838 - INFO - Received attributes: batch_size=12, num_epochs=9, max_sequence_len=15, learning_rate=1.0071984838809193e-06, weight_decay=0.009307782732027245
2024-03-15 11:26:09,839 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 11:26:10,920 - INFO - Tokenizer initialized successfully.
2024-03-15 11:26:10,921 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 11:26:15,164 - INFO - NER Dataset prepared successfully.
2024-03-15 11:26:15,165 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 11:26:17,146 - INFO - Model initialized successfully.
2024-03-15 11:26:17,147 - INFO - Training...
2024-03-15 11:40:57,961 - INFO - Training completed.
2024-03-15 11:40:57,962 - INFO - Evaluation...
2024-03-15 11:40:59,829 - INFO - Measured `f1` score: 0.4177997527812114
2024-03-15 11:40:59,830 - INFO - Evaluation completed.
2024-03-15 11:41:00,459 - INFO - Objective function execution completed in 890.62 seconds.
2024-03-15 11:41:00,461 - INFO - Starting (transformers) objective function execution.
2024-03-15 11:41:00,462 - INFO - Received attributes: batch_size=10, num_epochs=1, max_sequence_len=66, learning_rate=1.2366582530130827e-06, weight_decay=0.00012563152773938674
2024-03-15 11:41:00,463 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 11:41:01,649 - INFO - Tokenizer initialized successfully.
2024-03-15 11:41:01,651 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 11:41:05,842 - INFO - NER Dataset prepared successfully.
2024-03-15 11:41:05,842 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 11:41:07,853 - INFO - Model initialized successfully.
2024-03-15 11:41:07,854 - INFO - Training...
2024-03-15 11:43:30,039 - INFO - Training completed.
2024-03-15 11:43:30,040 - INFO - Evaluation...
2024-03-15 11:43:32,482 - INFO - Measured `f1` score: 0.15233092580433355
2024-03-15 11:43:32,483 - INFO - Evaluation completed.
2024-03-15 11:43:33,092 - INFO - Objective function execution completed in 152.63 seconds.
2024-03-15 11:43:33,094 - INFO - Starting (transformers) objective function execution.
2024-03-15 11:43:33,095 - INFO - Received attributes: batch_size=2, num_epochs=10, max_sequence_len=46, learning_rate=8.532678095658718e-06, weight_decay=2.3036990230378637e-06
2024-03-15 11:43:33,096 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 11:43:34,195 - INFO - Tokenizer initialized successfully.
2024-03-15 11:43:34,196 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 11:43:38,660 - INFO - NER Dataset prepared successfully.
2024-03-15 11:43:38,661 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 11:43:40,605 - INFO - Model initialized successfully.
2024-03-15 11:43:40,606 - INFO - Training...
2024-03-15 13:22:12,122 - INFO - Training completed.
2024-03-15 13:22:12,123 - INFO - Evaluation...
2024-03-15 13:22:17,888 - INFO - Measured `f1` score: 0.6317573931060463
2024-03-15 13:22:17,889 - INFO - Evaluation completed.
2024-03-15 13:22:18,441 - INFO - Objective function execution completed in 5925.35 seconds.
2024-03-15 13:22:18,443 - INFO - Starting (transformers) objective function execution.
2024-03-15 13:22:18,444 - INFO - Received attributes: batch_size=7, num_epochs=10, max_sequence_len=66, learning_rate=7.362945281639229e-05, weight_decay=0.0027527173929429443
2024-03-15 13:22:18,445 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 13:22:19,536 - INFO - Tokenizer initialized successfully.
2024-03-15 13:22:19,537 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 13:22:23,732 - INFO - NER Dataset prepared successfully.
2024-03-15 13:22:23,734 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 13:22:25,697 - INFO - Model initialized successfully.
2024-03-15 13:22:25,698 - INFO - Training...
2024-03-15 13:52:51,678 - INFO - Training completed.
2024-03-15 13:52:51,679 - INFO - Evaluation...
2024-03-15 13:52:54,508 - INFO - Measured `f1` score: 0.5809296343927981
2024-03-15 13:52:54,509 - INFO - Evaluation completed.
2024-03-15 13:52:55,127 - INFO - Objective function execution completed in 1836.68 seconds.
2024-03-15 13:52:55,130 - INFO - Starting (transformers) objective function execution.
2024-03-15 13:52:55,131 - INFO - Received attributes: batch_size=8, num_epochs=1, max_sequence_len=71, learning_rate=0.005872283616443731, weight_decay=0.00017912362571043672
2024-03-15 13:52:55,132 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 13:52:56,304 - INFO - Tokenizer initialized successfully.
2024-03-15 13:52:56,305 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 13:53:00,977 - INFO - NER Dataset prepared successfully.
2024-03-15 13:53:00,978 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 13:53:03,538 - INFO - Model initialized successfully.
2024-03-15 13:53:03,539 - INFO - Training...
2024-03-15 13:55:56,263 - INFO - Training completed.
2024-03-15 13:55:56,265 - INFO - Evaluation...
2024-03-15 13:55:58,875 - INFO - Measured `f1` score: 0.0
2024-03-15 13:55:58,876 - INFO - Evaluation completed.
2024-03-15 13:55:59,580 - INFO - Objective function execution completed in 184.45 seconds.
2024-03-15 13:55:59,582 - INFO - Starting (transformers) objective function execution.
2024-03-15 13:55:59,583 - INFO - Received attributes: batch_size=1, num_epochs=3, max_sequence_len=45, learning_rate=9.206654892274771e-06, weight_decay=0.0005408216580718496
2024-03-15 13:55:59,584 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 13:56:00,844 - INFO - Tokenizer initialized successfully.
2024-03-15 13:56:00,845 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 13:56:05,403 - INFO - NER Dataset prepared successfully.
2024-03-15 13:56:05,404 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 13:56:07,584 - INFO - Model initialized successfully.
2024-03-15 13:56:07,585 - INFO - Training...
2024-03-15 14:54:57,861 - INFO - Training completed.
2024-03-15 14:54:57,862 - INFO - Evaluation...
2024-03-15 14:55:08,023 - INFO - Measured `f1` score: 0.6073279052553663
2024-03-15 14:55:08,024 - INFO - Evaluation completed.
2024-03-15 14:55:08,570 - INFO - Objective function execution completed in 3548.99 seconds.
2024-03-15 14:55:08,572 - INFO - Starting (transformers) objective function execution.
2024-03-15 14:55:08,573 - INFO - Received attributes: batch_size=13, num_epochs=3, max_sequence_len=65, learning_rate=3.666421832063727e-05, weight_decay=5.3572800696018355e-06
2024-03-15 14:55:08,574 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 14:55:09,613 - INFO - Tokenizer initialized successfully.
2024-03-15 14:55:09,614 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 14:55:13,751 - INFO - NER Dataset prepared successfully.
2024-03-15 14:55:13,752 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 14:55:15,689 - INFO - Model initialized successfully.
2024-03-15 14:55:15,690 - INFO - Training...
2024-03-15 15:01:25,886 - INFO - Training completed.
2024-03-15 15:01:25,887 - INFO - Evaluation...
2024-03-15 15:01:28,459 - INFO - Measured `f1` score: 0.5813383600377002
2024-03-15 15:01:28,460 - INFO - Evaluation completed.
2024-03-15 15:01:29,174 - INFO - Objective function execution completed in 380.60 seconds.
2024-03-15 15:01:29,177 - INFO - Starting (transformers) objective function execution.
2024-03-15 15:01:29,178 - INFO - Received attributes: batch_size=7, num_epochs=3, max_sequence_len=78, learning_rate=0.00018655260217376844, weight_decay=1.3342990285187939e-06
2024-03-15 15:01:29,179 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 15:01:30,517 - INFO - Tokenizer initialized successfully.
2024-03-15 15:01:30,518 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 15:01:35,464 - INFO - NER Dataset prepared successfully.
2024-03-15 15:01:35,465 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 15:01:37,838 - INFO - Model initialized successfully.
2024-03-15 15:01:37,840 - INFO - Training...
2024-03-15 15:11:42,268 - INFO - Training completed.
2024-03-15 15:11:42,270 - INFO - Evaluation...
2024-03-15 15:11:45,395 - INFO - Measured `f1` score: 0.0
2024-03-15 15:11:45,396 - INFO - Evaluation completed.
2024-03-15 15:11:46,119 - INFO - Objective function execution completed in 616.94 seconds.
2024-03-15 15:11:46,844 - INFO - Starting (transformers) objective function execution.
2024-03-15 15:11:46,845 - INFO - Received attributes: batch_size=4, num_epochs=10, max_sequence_len=47, learning_rate=7.943907717680148e-06, weight_decay=1e-06
2024-03-15 15:11:46,846 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 15:11:48,194 - INFO - Tokenizer initialized successfully.
2024-03-15 15:11:48,195 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 15:11:53,879 - INFO - NER Dataset prepared successfully.
2024-03-15 15:11:53,880 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 15:11:55,655 - INFO - Model initialized successfully.
2024-03-15 15:11:55,656 - INFO - Training...
2024-03-15 16:06:19,103 - INFO - Training completed.
2024-03-15 16:06:19,122 - INFO - Evaluation...
2024-03-15 16:06:23,490 - INFO - Measured `f1` score: 0.6303661029548411
2024-03-15 16:06:23,492 - INFO - Evaluation completed.
2024-03-15 16:06:24,210 - INFO - Objective function execution completed in 3277.37 seconds.
2024-03-15 16:06:25,006 - INFO - Starting (transformers) objective function execution.
2024-03-15 16:06:25,008 - INFO - Received attributes: batch_size=2, num_epochs=10, max_sequence_len=14, learning_rate=5.040295953716418e-05, weight_decay=1.052684455222534e-05
2024-03-15 16:06:25,008 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 16:06:26,365 - INFO - Tokenizer initialized successfully.
2024-03-15 16:06:26,366 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 16:06:32,247 - INFO - NER Dataset prepared successfully.
2024-03-15 16:06:32,248 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 16:06:33,921 - INFO - Model initialized successfully.
2024-03-15 16:06:33,923 - INFO - Training...
2024-03-15 17:56:21,994 - INFO - Training completed.
2024-03-15 17:56:22,013 - INFO - Evaluation...
2024-03-15 17:56:29,232 - INFO - Measured `f1` score: 0.0
2024-03-15 17:56:29,233 - INFO - Evaluation completed.
2024-03-15 17:56:29,956 - INFO - Objective function execution completed in 6604.95 seconds.
2024-03-15 17:56:30,785 - INFO - Starting (transformers) objective function execution.
2024-03-15 17:56:30,787 - INFO - Received attributes: batch_size=3, num_epochs=10, max_sequence_len=53, learning_rate=7.135943287733809e-05, weight_decay=7.69080025968071e-06
2024-03-15 17:56:30,788 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 17:56:32,146 - INFO - Tokenizer initialized successfully.
2024-03-15 17:56:32,147 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 17:56:38,028 - INFO - NER Dataset prepared successfully.
2024-03-15 17:56:38,029 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 17:56:39,822 - INFO - Model initialized successfully.
2024-03-15 17:56:39,823 - INFO - Training...
2024-03-15 19:10:09,014 - INFO - Training completed.
2024-03-15 19:10:09,016 - INFO - Evaluation...
2024-03-15 19:10:14,528 - INFO - Measured `f1` score: 0.0
2024-03-15 19:10:14,530 - INFO - Evaluation completed.
2024-03-15 19:10:15,255 - INFO - Objective function execution completed in 4424.47 seconds.
2024-03-15 19:10:15,965 - INFO - Starting (transformers) objective function execution.
2024-03-15 19:10:15,966 - INFO - Received attributes: batch_size=12, num_epochs=9, max_sequence_len=89, learning_rate=3.02339632564005e-05, weight_decay=0.009170204912777718
2024-03-15 19:10:15,968 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 19:10:17,314 - INFO - Tokenizer initialized successfully.
2024-03-15 19:10:17,316 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 19:10:23,112 - INFO - NER Dataset prepared successfully.
2024-03-15 19:10:23,114 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 19:10:24,819 - INFO - Model initialized successfully.
2024-03-15 19:10:24,821 - INFO - Training...
2024-03-15 19:32:19,655 - INFO - Training completed.
2024-03-15 19:32:19,669 - INFO - Evaluation...
2024-03-15 19:32:22,562 - INFO - Measured `f1` score: 0.6082130965593785
2024-03-15 19:32:22,563 - INFO - Evaluation completed.
2024-03-15 19:32:23,289 - INFO - Objective function execution completed in 1327.32 seconds.
2024-03-15 19:32:24,057 - INFO - Starting (transformers) objective function execution.
2024-03-15 19:32:24,059 - INFO - Received attributes: batch_size=3, num_epochs=9, max_sequence_len=49, learning_rate=7.532159062843439e-06, weight_decay=5.533135829935303e-06
2024-03-15 19:32:24,061 - INFO - Initializing Tokenizer: AutoTokenizer.from_pretrained('xlm-roberta-base')
2024-03-15 19:32:25,436 - INFO - Tokenizer initialized successfully.
2024-03-15 19:32:25,437 - INFO - Preparing NER Dataset: NERDataset_transformers(tokenizer=tokenizer, config=config_data, max_sequence_len=max_sequence_len)
2024-03-15 19:32:31,143 - INFO - NER Dataset prepared successfully.
2024-03-15 19:32:31,145 - INFO - Initializing Model: XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(ner_dataset.dataset.labels))
2024-03-15 19:32:32,928 - INFO - Model initialized successfully.
2024-03-15 19:32:32,930 - INFO - Training...
2024-03-15 20:38:38,325 - INFO - Training completed.
2024-03-15 20:38:38,337 - INFO - Evaluation...
2024-03-15 20:38:43,753 - INFO - Measured `f1` score: 0.6382336289866013
2024-03-15 20:38:43,754 - INFO - Evaluation completed.
2024-03-15 20:38:44,482 - INFO - Objective function execution completed in 3980.42 seconds.
2024-03-15 20:38:45,225 - INFO - End: scikit-optimize `Gaussian-Process` optimization procedure.
2024-03-15 20:38:45,226 - INFO - Saving the scikit-optimize (bayesian optimization) results w/ filename
