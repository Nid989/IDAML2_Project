# derived using `bayes-optimization` (w/ n_calls=10)
BATCH_SIZE: 3
NUM_EPOCHS: 9 
MAX_SEQUENCE_LEN: 49
LEARNING_RATE: 7.532159062843439e-06
WEIGHT_DECAY: 5.533135829935303e-06
EARLY_STOPPING_THRESHOLD: 5

DATASET_NAME: "MultiCoNER2"
# `BC5CDR` or `MultiCoNER2`

MODEL_CHECKPOINT: "xlm-roberta-base"
MODEL_NAME: "xlm-roberta-base"

# MODEL_IMPLEMENTATION: "tokenclassification"
    
VERSION: "v1"
# `v1`: tokenclassification

PATH_TO_MODEL_OUTPUT_DIR: "./model_checkpoints/transformers"