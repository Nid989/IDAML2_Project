# derived using `bayes-optimization` (w/ n_calls=3)
BATCH_SIZE: 2
NUM_EPOCHS: 5
MAX_SEQUENCE_LEN: 190
LEARNING_RATE: 8.532678095658718e-06 
WEIGHT_DECAY: 2.3036990230378637e-06
EARLY_STOPPING_THRESHOLD: 5

DATASET_NAME: "MultiCoNER2"
# `BC5CDR` or `MultiCoNER2`

MODEL_CHECKPOINT: "xlm-roberta-base"
MODEL_NAME: "xlm-roberta-base"

MODEL_IMPLEMENTATION: "tokenclassification"
# `tokenclassification` or `tokenclassification w/ LC-CRF" or `ensemble tokenclassification w/ and w/o LC-CRF`
    
VERSION: "v1"
# `v1`: tokenclassification
# `v2`: tokenclassification w/ LC-CRF

PATH_TO_MODEL_OUTPUT_DIR: "./model_checkpoints/transformers"